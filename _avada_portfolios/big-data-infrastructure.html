---
layout: avada_portfolio
title: Big Data Infrastructure
date: 
type: avada_portfolio
parent_id: '0'
published: false
password: ''
status: private
categories: []
tags: []
meta:
  _edit_last: '2'
  sbg_selected_sidebar_replacement: a:1:{i:0;s:1:"0";}
  slide_template: default
  pyre_width: full
  pyre_portfolio_width_100: 'no'
  pyre_sidebar: 'no'
  pyre_sidebar_position: default
  pyre_project_desc_title: 'yes'
  pyre_project_details: 'yes'
  pyre_show_first_featured_image: 'no'
  pyre_video: ''
  pyre_video_url: ''
  pyre_project_url: ''
  pyre_project_url_text: ''
  pyre_copy_url: ''
  pyre_copy_url_text: ''
  pyre_fimg_width: ''
  pyre_fimg_height: ''
  pyre_image_rollover_icons: linkzoom
  pyre_link_icon_url: ''
  pyre_link_icon_target: 'no'
  pyre_related_posts: 'yes'
  pyre_slider_type: 'no'
  pyre_slider: '0'
  pyre_wooslider: '0'
  pyre_flexslider: '0'
  pyre_revslider: '0'
  pyre_elasticslider: '0'
  pyre_fallback: ''
  pyre_page_bg_layout: default
  pyre_page_bg: ''
  pyre_page_bg_color: ''
  pyre_page_bg_full: 'no'
  pyre_page_bg_repeat: repeat
  pyre_wide_page_bg: ''
  pyre_wide_page_bg_color: ''
  pyre_wide_page_bg_full: 'no'
  pyre_wide_page_bg_repeat: repeat
  pyre_header_bg: ''
  pyre_header_bg_color: ''
  pyre_header_bg_full: 'no'
  pyre_header_bg_repeat: repeat
  pyre_page_title: 'yes'
  pyre_page_title_text: 'yes'
  pyre_page_title_custom_text: ''
  pyre_page_title_custom_subheader: ''
  pyre_page_title_height: ''
  pyre_page_title_bar_bg: ''
  pyre_page_title_bar_bg_retina: ''
  pyre_page_title_bar_bg_full: default
  pyre_page_title_bar_bg_color: ''
  pyre_page_title_bg_parallax: default
  _thumbnail_id: '5361'
  sbg_selected_sidebar: a:1:{i:0;s:1:"0";}
author:
  login: sychoi
  email: alicia.sychoi@gmail.com
  display_name: sychoi
  first_name: ''
  last_name: ''
permalink: "/portfolio-items/big-data-infrastructure/"
---
<p>We are building an infrastructure for big data engines. Although the data processing models of big data engines differ, they all face a fundamental set of challenges in a distributed environment such as configuration, fault tolerance, and in-memory data management. Our big data infrastructure aims to provide a general abstraction that deals with such concerns under the hood so that big data engines can focus on what they do best.</p>
<p><strong>Big Data OS Core: REEF</strong><br />
REEF (Retainable Evaluator Execution Framework) is a scale-out computing fabric that makes it easier to write Big Data applications on top of resource managers (e.g., Apache YARN and Mesos). With a goal to simplify and unify the lower layers of big data analytics frameworks on resource managers, it provides building blocks commonly used in such systems including configuration, resource management, and error handling. REEF is an open source project; from August 2014, REEF has been incubated under Apache Software Foundation. Developers from various organizations such as SNU, Microsoft, UCLA, UW, and PureStorage are working together in contributing to the project.</p>
<p><strong>Building Blocks: Wake, Tang</strong><br />
Wake is an event-driven framework based on ideas from SEDA, Click, Akka, and Rx. It is general purpose in the sense that it is designed to support computationally intensive applications as well as high performance networking, storage, and legacy I/O systems. We implemented Wake to support high-performance, scalable “big-data" systems, and have used it to implement control plane logic and the data plane of REEF. Tang is a configuration management and checking framework that emphasizes explicit documentation and automatic checkability of configurations and applications instead of ad-hoc, application-specific configurations. Tang makes use of "dependency injection" to automatically instantiate applications. By doing this, Tang can catch configuration errors before applied, which prevents catastrophic failure of whole system in advance. It supports distributed, multi-language applications, but gracefully handles simpler use cases as well.</p>
<p><strong>Big Data OS Services: Elastic Memory</strong><br />
For simplicity, recent in-memory big data systems deploy workers on a static allocation of the cluster resources. This simplicity comes at a cost: the unpredictability in the size of input data can severely damage performance and resource utilization. To bring back elasticity to big data analytics, we are building Elastic Memory on REEF, an abstraction that can dynamically change the allocated memory resource with an aim to improve cluster resource utilization and job performance.</p>
<p><strong>Unified Data Management: Surf</strong><br />
During a pipelined big data analytics job, data travels through multiple storage layers. But because each storage layer has different assumptions about data and is unaware of the end-to-end workflow, a lot of unnecessary overhead occurs. We aim to unify multiple layers of storage in a way that minimizes such overhead and provide a clean interface for managing data. As a first step towards this goal, we’re developing Surf, an in-memory caching tier for distributed file systems such as HDFS.</p>
<p>&nbsp;</p>
